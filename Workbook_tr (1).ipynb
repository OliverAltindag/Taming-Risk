{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d2-K5lmxtTF"
      },
      "source": [
        "##**Pips**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brni2puIubTJ"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install --upgrade yfinance\n",
        "!pip show yfinance\n",
        "!pip install emcee # astronomy mcmc runner, sorry!\n",
        "!pip install emcee --upgrade\n",
        "!pip install corner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRzwrbexxge"
      },
      "source": [
        "##**Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVwOrNMTvdYm"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from numpy.linalg import norm\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import matplotlib.cm as cm\n",
        "import requests\n",
        "import emcee\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QWr0QquwwhB"
      },
      "outputs": [],
      "source": [
        "# Get the list of S&P 500 tickers from Wikipedia, the most effective for what we are doing\n",
        "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "sp500_table = pd.read_html(sp500_url, header=0)[0]\n",
        "\n",
        "# Extract tickers from the 'Symbol' column\n",
        "sp500_tickers = sp500_table['Symbol'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccq3PYdInQjf"
      },
      "outputs": [],
      "source": [
        "# Sets tickers to a with an adjustable number of them selected\n",
        "n = 11\n",
        "assets = np.random.choice(sp500_tickers, n) # actaully gets them\n",
        "print(assets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds86BQenZubR"
      },
      "outputs": [],
      "source": [
        "# returns all of the needed ticker data for our analyses\n",
        "def expected_return(tickers):\n",
        "    expected_returns_avg = pd.DataFrame() # empty df\n",
        "    returns_quarterly = pd.DataFrame() # empty df\n",
        "    dropped_tickers = [] # empty list\n",
        "\n",
        "    # Define time range: from 2020 Q1 to just before 2024\n",
        "    start_date = pd.to_datetime(\"2020-01-01\").tz_localize(\"America/New_York\") # want to start it in 2020 (start of covid)\n",
        "    cutoff_date = pd.to_datetime(\"2024-01-01\").tz_localize(\"America/New_York\") # end the firts day fo 2024, so we can predict the end of 2025 evaluation\n",
        "\n",
        "    for ticker in tickers: # these are determined in the cell above\n",
        "        try:\n",
        "            # calls and gets the ticker data using yfinance\n",
        "            obj = yf.Ticker(ticker)\n",
        "            symbol = obj.info.get('symbol',ticker)\n",
        "            hist = obj.history(period=\"10y\",auto_adjust=True,interval=\"3mo\") # set to take quarterly data for a longer period than we will need, later is cut down\n",
        "\n",
        "            if hist.empty or 'Close' not in hist.columns: # notes the tickers which could not have their data fetched\n",
        "                dropped_tickers.append(ticker)\n",
        "                continue\n",
        "\n",
        "            # tehcnical code bc the timezome was causing errors, this is important when we drop data based on dates\n",
        "            if hist.index.tz is None:\n",
        "                hist = hist.tz_localize(\"America/New_York\")\n",
        "            else:\n",
        "                hist = hist.tz_convert(\"America/New_York\")\n",
        "\n",
        "            # Filter to be all quarterly 2020-2023 data, note these are based off variables we made earlier in the code\n",
        "            hist = hist[(hist.index>=start_date)&(hist.index<cutoff_date)]\n",
        "\n",
        "            if hist.empty: # again to note tickers which were dropped bc of the new date constraints\n",
        "                dropped_tickers.append(ticker)\n",
        "                continue\n",
        "\n",
        "            # calculate returns\n",
        "            return_quarterly = hist['Close'].pct_change() # takes the close on the day\n",
        "            returns_quarterly[symbol] = return_quarterly # sets it to the specific ticker\n",
        "\n",
        "        except Exception as e: # to visualize the dropped tickers in our process, and for when others use the notebook it will be apparent their random generation was not perfectly usable\n",
        "            print(f\"Skipping {ticker} due to error: {e}\")\n",
        "            dropped_tickers.append(ticker)\n",
        "            continue\n",
        "\n",
        "    # Handle missing data\n",
        "    returns_quarterly = returns_quarterly.ffill().bfill()  # fixes deprecated fillna\n",
        "    all_nan_cols = returns_quarterly.columns[returns_quarterly.isna().all()] # if any have completely missing columns, drop\n",
        "    dropped_tickers.extend(all_nan_cols.tolist()) # add tickers that were dropped to report them, again\n",
        "    returns_quarterly.dropna(axis=1,how='all',inplace=True) # drop any that still are all na just incase\n",
        "\n",
        "    # Calculate expected returns\n",
        "    expected_returns_avg[0] = returns_quarterly.mean(axis=0) # the mean of all of the quarterly returns\n",
        "    print(f\"Dropped tickers: {dropped_tickers}\") # view the dropped tickers\n",
        "    return expected_returns_avg, returns_quarterly # returns the final needed data points for our optimization\n",
        "\n",
        "\n",
        "# just prints an example, never used again really, just to test the waters and see what the data is looking like\n",
        "expected_returns_avg, returns_quarterly = expected_return(assets)\n",
        "returns = expected_returns_avg.values.flatten()\n",
        "print(expected_returns_avg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_antZfJZFsB"
      },
      "source": [
        "##**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aurKZmXZ_a9"
      },
      "outputs": [],
      "source": [
        "# Intialize the weights\n",
        "n_assets = len(expected_returns_avg)\n",
        "weight = np.ones(n_assets,dtype=float) # Initialize with dtype=float, and use the correct name for weights here\n",
        "weight /= weight.sum()\n",
        "\n",
        "# defining the portfolio_return function\n",
        "def portfolio_return(weights, returns):\n",
        "    portfolio_return = weights@returns  # how its actually calculated\n",
        "    return portfolio_return\n",
        "\n",
        "# covariance matrix for the quarterly data\n",
        "returns_cov = returns_quarterly.cov()  # covariance matrix fo the quarterly data\n",
        "\n",
        "# to plot and visualize this initial covariance matrix that we base so much on\n",
        "fig, ax = plt.subplots(figsize=(n_assets, n_assets))\n",
        "sb.heatmap(returns_cov,annot=True,cmap='coolwarm')  # formatting and consistent color scheme\n",
        "plt.show()\n",
        "\n",
        "# defining the portfolio_variance, and adding the cov_matrix as an argument\n",
        "def portfolio_variance(weights,cov_matrix):\n",
        "    return weights.T@cov_matrix@weights  # just the math behind it using matrix multiplication\n",
        "\n",
        "# for visualization of the data we will be using in the optimizer, and again in final comparison after MCMC\n",
        "print(\"initial-weights:\", weight) # Make sure the weight is in the correct format or shape for cov_matrix\n",
        "print(\"portfolio_variance:\", portfolio_variance(weight,returns_cov))\n",
        "print(\"portfolio_return:\", portfolio_return(weight,expected_returns_avg)) # make sure weight is either converted or returns is changed to returns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the objective function\n",
        "def sharpe_objective(weights,mu,cov):\n",
        "    portfolio_return = weights@mu # the portfolio return in terms of the stock weights and the mean returns\n",
        "    portfolio_volatility = np.sqrt(weights@cov@ weights.T) # the variability in terms of the weights and covariance matrix of the quarterly returns\n",
        "    return -(portfolio_return / portfolio_volatility)  # negative sharpe ratio for minimization by gradient descent\n",
        "\n",
        "# gradient of the objective function\n",
        "def sharpe_gradient(weights, mu, cov):\n",
        "    port_return = weights@mu # repeating what's above so that can reference inside the function\n",
        "    port_volatility = np.sqrt(weights@ cov@ weights.T) # same as above\n",
        "    # below is the actual gradient computation\n",
        "    numerator = mu*port_volatility-(port_return/port_volatility) *cov@weights # numerator of the gradient\n",
        "    denominator = port_volatility**2 # denominator of the gradient\n",
        "    return -(numerator / denominator)  # negative gradient for minimization\n",
        "\n",
        "# the optimizer\n",
        "def gradient_descent(mu, cov, initial_weights, step_size=0.01, threshold=1e-5, max_iterations=10000):\n",
        "\n",
        "    # diagonal scaling technique, adapted bc the normal one did not always work\n",
        "    std_devs = np.sqrt(np.diag(cov)) # get standard deviations of each asset\n",
        "    D = np.diag(1.0 / std_devs) # create inverse volatility scaling matrix\n",
        "    mu_scaled = D @ mu # scale expected returns\n",
        "    cov_scaled = D @ cov @ D # scale covariance matrix\n",
        "\n",
        "    # scaled intial weights\n",
        "    x_initial = D @ initial_weights # bang, done right here\n",
        "    x_initial /= x_initial.sum() # normalized scaled weights\n",
        "\n",
        "    x = x_initial.copy().astype(float) # calling these initial weights from the previous cell\n",
        "    iteration = 0 # the first iteration is zero\n",
        "    # bunch of empty lists to store values in for visualization and analysis\n",
        "    objective_values = []\n",
        "    weights_history = []\n",
        "    portfolio_variances = []\n",
        "    portfolio_returns = []\n",
        "\n",
        "    while iteration < max_iterations: # stopping criterion (outer loop)\n",
        "        grad = sharpe_gradient(x, mu_scaled, cov_scaled) # grad\n",
        "        grad_norm = np.linalg.norm(grad) # norm\n",
        "\n",
        "        if grad_norm < threshold: # stopping criterion which is actually used, almost always will be this one\n",
        "            print(f\"Converged after {iteration} iterations.\")\n",
        "            break # actually stops the optimization once it does\n",
        "\n",
        "        # gradient descent\n",
        "        direction = -grad  # normal gradient descent syntax and theory\n",
        "        x = x + step_size * direction # updates the scaled weights\n",
        "        x /= x.sum() # renormalize\n",
        "\n",
        "        # transform BACK to original weight space so the values are correct\n",
        "        weights = D @ x\n",
        "        weights /= weights.sum() # renormalize in the regular space\n",
        "\n",
        "        # Record metrics\n",
        "        current_objective = sharpe_objective(weights,mu,cov) # the values of the objective as iterations go by\n",
        "        objective_values.append(current_objective)\n",
        "        weights_history.append(weights.copy()) # the weights as time goes by\n",
        "\n",
        "        # calculates portfolio return and variance\n",
        "        portfolio_return = weights@mu # same as at the beginning but updated values\n",
        "        portfolio_variance = weights@cov@weights.T # here too\n",
        "\n",
        "        portfolio_returns.append(portfolio_return) # adds the new values\n",
        "        portfolio_variances.append(portfolio_variance) # same as above\n",
        "        iteration += 1\n",
        "\n",
        "    if iteration == max_iterations: # if it hits the max iteration have this, to keep runtime down\n",
        "        print(\"Reached maximum number of iterations.\")\n",
        "\n",
        "    return weights, iteration, objective_values, weights_history, portfolio_variances, portfolio_returns # returns all of the optimized values sometimes this will return weights which are just straight lines"
      ],
      "metadata": {
        "id": "nVhJvm6sIkcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5DNwsZKJ0MP"
      },
      "outputs": [],
      "source": [
        "# bc we are doing this for any portfolio, these need to be played with for each one, but realtively similar: they are all their own problem with different\n",
        "# had to use const step size, bc exact line search and backtracking did not work for all portfolios bc of bad gradient functions but constant step did which we determined after extensive testing\n",
        "# runs the optimizer\n",
        "optimized_weights, iterations, objective_values, weights_history, portfolio_variances, portfolio_returns = gradient_descent(\n",
        "    mu=returns,\n",
        "    cov=returns_cov,\n",
        "    initial_weights=weight,\n",
        "    step_size=0.01, # lower this if oscillates and it will converge, this may also need to be increased depending on the portfolio some are extremely ill conditioned, this is just what happened to work for the last one ran\n",
        "    threshold=1e-4,\n",
        "    max_iterations=10000 # also may have to increase this is the convergence looks almost done but is not\n",
        ")\n",
        "\n",
        "# calculates the sharpe ratios for before and after the optimizer\n",
        "initial_sharpe = portfolio_return(weight, returns)/np.sqrt(portfolio_variance(weight, returns_cov))\n",
        "optimized_sharpe = portfolio_return(optimized_weights, returns)/np.sqrt(portfolio_variance(optimized_weights, returns_cov))\n",
        "\n",
        "# builds output table\n",
        "data = [\n",
        "    [\"initial_weights\", weight.tolist()],\n",
        "    [\"initial_portfolio_return\", portfolio_return(weight, returns)],\n",
        "    [\"initial_portfolio_variance\", portfolio_variance(weight, returns_cov)],\n",
        "    [\"initial_portfolio_sharpe\", initial_sharpe],\n",
        "    [\"optimized_weights\", optimized_weights.tolist()],\n",
        "    [\"optimized_portfolio_return\", portfolio_return(optimized_weights, returns)],\n",
        "    [\"optimized_portfolio_variance\", portfolio_variance(optimized_weights, returns_cov)],\n",
        "    [\"optimized_portfolio_sharpe\", optimized_sharpe],\n",
        "    [\"optimized_objective_value\", sharpe_objective(optimized_weights, returns, returns_cov)], # Assuming sharpe_objective is the intended objective function\n",
        "    [\"number_of_iterations\", iterations],\n",
        "    [\"initial_sharpe\", initial_sharpe],\n",
        "    [\"optimized_sharpe\", optimized_sharpe]\n",
        "]\n",
        "\n",
        "# Create and display DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"parameters\", \"Values\"])\n",
        "print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozd6erOIL_7F"
      },
      "outputs": [],
      "source": [
        "#plot the graphs returns vs risk\n",
        "\n",
        "expected_returns, values = expected_return(assets)  # returns from live market of the stocks\n",
        "variances = np.diagonal(returns_cov)  # variance of each data assets\n",
        "\n",
        "valid_assets = expected_returns.index.tolist()\n",
        "\n",
        "for i, asset in enumerate(valid_assets):\n",
        "    plt.annotate(asset, (variances[i], expected_returns.loc[asset].values[0]))  # text\n",
        "\n",
        "plt.scatter(portfolio_variance(weight,returns_cov),portfolio_return(weight,returns),label=\"initial portfolio value\")\n",
        "plt.scatter(portfolio_variance(optimized_weights,returns_cov), portfolio_return(optimized_weights,returns), color=\"red\", label=\"optimized portfolio value\")\n",
        "plt.scatter(variances,returns)\n",
        "plt.xlabel(\"Risk\")\n",
        "plt.ylabel(\"Returns\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCkmKtcQFWhJ"
      },
      "outputs": [],
      "source": [
        "# objective function vs iterations\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(objective_values)),objective_values,linestyle='-', color='blue')\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Objective Function Value\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# weigts vs iterations\n",
        "# makes the weights an array for plotting purposes\n",
        "weights_array = np.array(weights_history)\n",
        "cmap = cm.get_cmap('coolwarm',weights_array.shape[1])  # number of assets, also a consistent coloring\n",
        "# the actual plotting code\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(weights_array.shape[1]):\n",
        "    plt.plot(weights_array[:, i],label=assets[i],color=cmap(i))\n",
        "\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Asset Weight\")\n",
        "plt.legend(loc='best', fontsize='small')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwcgTFmjUJF"
      },
      "source": [
        "##**MCMC Forecasting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuBmCqDWmGh5"
      },
      "outputs": [],
      "source": [
        "weights = optimized_weights.copy() # use the optimized weigths\n",
        "\n",
        "# gets the recent returns\n",
        "recent_returns = returns_quarterly.iloc[-40:] # gets more data than I even have out of laziness, they wont get plotted regardless\n",
        "n_quarters,n_assets = recent_returns.shape # gets the shape\n",
        "\n",
        "# computes the covariance matrix from historical data (again), just recalling the variable I used earlier in the code\n",
        "returns_cov = recent_returns.cov().values\n",
        "\n",
        "# portfolio volatility same as what was done before\n",
        "port_volatility = np.sqrt(weights@returns_cov@weights.T)\n",
        "\n",
        "# empty list for the empty list\n",
        "sharpe_history = []\n",
        "\n",
        "for t in range(n_quarters): # the number of quarters that I actually have data for, but on a roling window\n",
        "    quarter_return = recent_returns.iloc[t].values # gets the data\n",
        "    port_return = weights@quarter_return # same as in the optimizer\n",
        "    ratio = port_return/port_volatility  # here as well\n",
        "    sharpe_history.append(ratio) # appends the empty list so can use the values later one\n",
        "\n",
        "# convert it to numpy array so I can plot it and analyze it in mcmc\n",
        "sharpe_history = np.array(sharpe_history)\n",
        "\n",
        "# plots it, lot of little fromatting things in there as well\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(sharpe_history, 'ko-', label='Quarterly Sharpe Ratio')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Sharpe Ratio')\n",
        "# fomratting of the tick marks ot make it look nicer and actually be seen\n",
        "plt.xticks(\n",
        "    np.arange(len(recent_returns)),\n",
        "    [f\"{d.year}-Q{((d.month - 1)// 3) + 1}\" for d in recent_returns.index],\n",
        "    rotation=45\n",
        ")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlF26ADSWNi6"
      },
      "outputs": [],
      "source": [
        "random.seed(11)\n",
        "# Define data\n",
        "x = np.arange(len(sharpe_history)) # gets the number of quarter\n",
        "y = sharpe_history # the actual ratios\n",
        "N = len(x)\n",
        "\n",
        "# continuous piecewise linear model with 3 segments and adaptive slopes and changepoints, no data input\n",
        "def piecewise(x, cp1, cp2, m1, b1, m2, m3): # the initial parameters\n",
        "    b2 = (m1 - m2) *cp1 +b1 # the intercept so that it conencts, second line\n",
        "    b3 = (m2 - m3) *cp2 +b2 # here but for the third line\n",
        "    y_model = np.zeros_like(x, dtype=float) # empty to add values into later\n",
        "    for i, xi in enumerate(x): # loop over the x values to find the y values\n",
        "        if xi < cp1:\n",
        "            y_model[i] = m1*xi+b1 # before change 1\n",
        "        elif xi < cp2:\n",
        "            y_model[i] = m2*xi+b2 # before change 2\n",
        "        else:\n",
        "            y_model[i] = m3*xi+b3 # final line\n",
        "    return y_model\n",
        "\n",
        "# log-likelihood with added uncertainty so that it fits the data more loosely, bc if not just matches very short periods not general trends\n",
        "def log_likelihood(theta, x, y):\n",
        "    cp1, cp2, m1, b1, m2, m3 = theta # unpacks the input\n",
        "    cp1 = np.clip(cp1, 1.0, N - 3 - 1e-6) # clips unrealistic change points\n",
        "    cp2 = np.clip(cp2, cp1 + 1e-6, N - 2 - 1e-6) # clips unrealistic change points\n",
        "    model = piecewise(x, cp1, cp2,m1, b1, m2, m3) # references the function defined above to actually get the lines\n",
        "    sigma = 0.05 # noise is added with this, made a variable so can play around with it to find the best fits\n",
        "    return -0.5 * np.sum(((y - model)/sigma)**2 + np.log(2 *np.pi* sigma ** 2)) # gaussian normal noise assumption, but taking the log\n",
        "\n",
        "# tightened log-prior to avoid multimodal behavior\n",
        "def log_prior(theta):\n",
        "    cp1, cp2, m1, b1, m2, m3 = theta # againunpack the\n",
        "    # Changepoint constraints (stronger middle restriction)\n",
        "    if not (0.2 * N <= cp1 <cp2 <= 0.8 * N):  # Narrower bounds\n",
        "        return -np.inf\n",
        "    # slope constraints, relatively tight to encourage shallower slopes\n",
        "    if not (-.5 < m1 < .5 and -.5 < m2 < .5 and -.5 < m3 < .5):\n",
        "        return -np.inf\n",
        "    # intercept constraints, again little closer ot reduce extremity\n",
        "    if not (-5 < b1 < 5):\n",
        "        return -np.inf\n",
        "    return 0.0\n",
        "\n",
        "# Posterior function, using the fucntions above\n",
        "def log_posterior(theta, x, y):\n",
        "    lp = log_prior(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    return lp+log_likelihood(theta, x, y) # log base so can add them like this, common emcee practice to frmulate like this for stability\n",
        "\n",
        "# initialization\n",
        "init_guess = [N/3, 2*N/3, 0.05, y[0], 0.05, 0.05]  # eh guess\n",
        "pos = init_guess + 1e-4 * np.random.randn(36, 6)  # eh guess\n",
        "\n",
        "# run MCMC, big burnin incase (sorry I use the astronomy mcmc package)\n",
        "sampler = emcee.EnsembleSampler(36, 6, log_posterior, args=(x, y))\n",
        "sampler.run_mcmc(pos, 10000, progress=True)\n",
        "\n",
        "# extract best fit\n",
        "flat_samples = sampler.get_chain(discard=5000, thin=10, flat=True)\n",
        "best_idx = np.argmax([log_posterior(p, x, y) for p in flat_samples]) # the data\n",
        "best_fit = flat_samples[best_idx] # flattens it for easy access\n",
        "cp1, cp2, m1, b1, m2, m3 = best_fit # the values\n",
        "cp1 = np.clip(cp1, 1.0, N - 3 - 1e-6) # ensures not too close to the beginning or the end to not just follow the extremely localized trend\n",
        "cp2 = np.clip(cp2, cp1 + 1e-6, N - 2 - 1e-6) # here as well bbut the second changepoint\n",
        "\n",
        "# forecast settings\n",
        "QUARTERS_AHEAD = 4 # we played aroung with this, but one year made the most logical sense, hence 4 quarters\n",
        "x_forecast = np.arange(N, N + QUARTERS_AHEAD)  # just simply doing the initialization with\n",
        "num_samples = flat_samples.shape[0] # gets the samples and indexing needed terms\n",
        "predictions = np.zeros((num_samples, QUARTERS_AHEAD)) # empty array of the correct\n",
        "\n",
        "# generate predictions for all posterior samples\n",
        "for i, theta in enumerate(flat_samples):\n",
        "    cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s = theta # unpacks the variables\n",
        "    cp1_s = np.clip(cp1_s, 1.0, N - 3 - 1e-6) # same clipping\n",
        "    cp2_s = np.clip(cp2_s, cp1_s + 1e-6, N - 2 - 1e-6) # here aswell\n",
        "    y_pred = piecewise(x_forecast, cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s) # calls the fucntion fro it again\n",
        "    predictions[i] = y_pred # gets the predictions\n",
        "\n",
        "# compute CI from the bayesian inference\n",
        "ci_lower = np.percentile(predictions,2.5,axis=0)\n",
        "ci_upper = np.percentile(predictions,97.5,axis=0)\n",
        "median_forecast = np.median(predictions,axis=0) # this is what is used as the predicted val.\n",
        "\n",
        "# evaluate best-fit model up to 2025\n",
        "x_full = np.arange(N+4)  # through 2025\n",
        "best_fit_model = piecewise(x_full, cp1, cp2, m1, b1, m2, m3) # again putting into the function\n",
        "\n",
        "# plot prediction with CI and mcmc chains to see it better\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(x, y, 'ko-', label='Historical Sharpe Ratio')\n",
        "plt.plot(x_full, best_fit_model, 'r-', lw=2, label='Best Fit') # for the fitted data\n",
        "\n",
        "# plot posterior samples of the predictions, like the CI (kinda)\n",
        "for i, theta in enumerate(flat_samples[:50]):\n",
        "    cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s = theta\n",
        "    cp1_s = np.clip(cp1_s, 1.0, N - 3 - 1e-6) # the same clipping as always\n",
        "    cp2_s = np.clip(cp2_s, cp1_s + 1e-6, N - 2 - 1e-6)\n",
        "    y_pred = piecewise(x_forecast, cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s) # calls the function once more\n",
        "    plt.plot(x_forecast, y_pred, 'b-', alpha=0.1, lw=1) # plots\n",
        "\n",
        "# plot median forecast and CI\n",
        "plt.plot(x_forecast, median_forecast, 'b',lw=2, label='Median Forecast') # our predicted value\n",
        "plt.fill_between(x_forecast, ci_lower, ci_upper, color='blue', alpha=0.2, label='95% CI') # the corresponding CI\n",
        "\n",
        "# highlight current and predicted point, so know where it is visually\n",
        "plt.scatter([x_forecast[-1]], [median_forecast[-1]], color='orange', zorder=5,\n",
        "            label=f'Predicted: {median_forecast[-1]:.3f}')\n",
        "\n",
        "# vertical lines, pretty simple code but just makes a vertical line for the end of fitting then predicted model endpoint\n",
        "plt.axvline(N - 1, color='green',linestyle=':', label='End of 2023')\n",
        "plt.axvline(x_forecast[-1], color='purple',linestyle='--', label='Start of 2025')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Sharpe Ratio')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# to observe the exact values for writing the report, all fust referencing the correct stuff above\n",
        "print(f\"Predicted Sharpe Ratio at Q19: {median_forecast[-1]:.3f}\")\n",
        "print(f\"95% Confidence Interval: [{ci_lower[-1]:.3f}, {ci_upper[-1]:.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wt2i47Xyqj-"
      },
      "outputs": [],
      "source": [
        "weights = weight.copy() # use the optimized weigths\n",
        "\n",
        "# gets the recent returns\n",
        "recent_returns = returns_quarterly.iloc[-40:] # gets more data than I even have out of laziness, they wont get plotted regardless\n",
        "n_quarters,n_assets = recent_returns.shape # gets the shape\n",
        "\n",
        "# computes the covariance matrix from historical data (again), just recalling the variable I used earlier in the code\n",
        "returns_cov = recent_returns.cov().values\n",
        "\n",
        "# portfolio volatility same as what was done before\n",
        "port_volatility = np.sqrt(weights@returns_cov@weights.T)\n",
        "\n",
        "# empty list for the empty list\n",
        "sharpe_historyi = []\n",
        "\n",
        "for t in range(n_quarters): # the number of quarters that I actually have data for, but on a roling window\n",
        "    quarter_return = recent_returns.iloc[t].values # gets the data\n",
        "    port_return = weights@quarter_return # same as in the optimizer\n",
        "    ratio = port_return/port_volatility  # here as well\n",
        "    sharpe_historyi.append(ratio) # appends the empty list so can use the values later one\n",
        "\n",
        "# convert it to numpy array so I can plot it and analyze it in mcmc\n",
        "sharpe_historyi = np.array(sharpe_historyi)\n",
        "\n",
        "# plots it, lot of little fromatting things in there as well\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(sharpe_historyi, 'ko-', label='Quarterly Sharpe Ratio')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Sharpe Ratio')\n",
        "# fomratting of the tick marks ot make it look nicer and actually be seen\n",
        "plt.xticks(\n",
        "    np.arange(len(recent_returns)),\n",
        "    [f\"{d.year}-Q{((d.month - 1)// 3) + 1}\" for d in recent_returns.index],\n",
        "    rotation=45\n",
        ")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rQEdsY_zLBI"
      },
      "outputs": [],
      "source": [
        "random.seed(11)\n",
        "# Define data\n",
        "x = np.arange(len(sharpe_historyi)) # gets the number of quarter\n",
        "y = sharpe_historyi # the actual ratios\n",
        "N = len(x)\n",
        "\n",
        "# continuous piecewise linear model with 3 segments and adaptive slopes and changepoints, no data input\n",
        "def piecewise(x, cp1, cp2, m1, b1, m2, m3): # the initial parameters\n",
        "    b2 = (m1 - m2) *cp1 +b1 # the intercept so that it conencts, second line\n",
        "    b3 = (m2 - m3) *cp2 +b2 # here but for the third line\n",
        "    y_model = np.zeros_like(x, dtype=float) # empty to add values into later\n",
        "    for i, xi in enumerate(x): # loop over the x values to find the y values\n",
        "        if xi < cp1:\n",
        "            y_model[i] = m1*xi+b1 # before change 1\n",
        "        elif xi < cp2:\n",
        "            y_model[i] = m2*xi+b2 # before change 2\n",
        "        else:\n",
        "            y_model[i] = m3*xi+b3 # final line\n",
        "    return y_model\n",
        "\n",
        "# log-likelihood with added uncertainty so that it fits the data more loosely, bc if not just matches very short periods not general trends\n",
        "def log_likelihood(theta, x, y):\n",
        "    cp1, cp2, m1, b1, m2, m3 = theta # unpacks the input\n",
        "    cp1 = np.clip(cp1, 1.0, N - 3 - 1e-6) # clips unrealistic change points\n",
        "    cp2 = np.clip(cp2, cp1 + 1e-6, N - 2 - 1e-6) # clips unrealistic change points\n",
        "    model = piecewise(x, cp1, cp2,m1, b1, m2, m3) # references the function defined above to actually get the lines\n",
        "    sigma = 0.05 # noise is added with this, made a variable so can play around with it to find the best fits\n",
        "    return -0.5 * np.sum(((y - model)/sigma)**2 + np.log(2 *np.pi* sigma ** 2)) # gaussian normal noise assumption, but taking the log\n",
        "\n",
        "# tightened log-prior to avoid multimodal behavior\n",
        "def log_prior(theta):\n",
        "    cp1, cp2, m1, b1, m2, m3 = theta # againunpack the\n",
        "    # Changepoint constraints (stronger middle restriction)\n",
        "    if not (0.2 * N <= cp1 <cp2 <= 0.8 * N):  # Narrower bounds\n",
        "        return -np.inf\n",
        "    # slope constraints, relatively tight to encourage shallower slopes\n",
        "    if not (-.5 < m1 < .5 and -.5 < m2 < .5 and -.5 < m3 < .5):\n",
        "        return -np.inf\n",
        "    # intercept constraints, again little closer ot reduce extremity\n",
        "    if not (-5 < b1 < 5):\n",
        "        return -np.inf\n",
        "    return 0.0\n",
        "\n",
        "# Posterior function, using the fucntions above\n",
        "def log_posterior(theta, x, y):\n",
        "    lp = log_prior(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    return lp+log_likelihood(theta, x, y) # log base so can add them like this, common emcee practice to frmulate like this for stability\n",
        "\n",
        "# initialization\n",
        "init_guess = [N/3, 2*N/3, 0.05, y[0], 0.05, 0.05]  # eh guess\n",
        "pos = init_guess + 1e-4 * np.random.randn(36, 6)  # eh guess\n",
        "\n",
        "# run MCMC, big burnin incase (sorry I use the astronomy mcmc package)\n",
        "sampler = emcee.EnsembleSampler(36, 6, log_posterior, args=(x, y))\n",
        "sampler.run_mcmc(pos, 10000, progress=True)\n",
        "\n",
        "# extract best fit\n",
        "flat_samples = sampler.get_chain(discard=5000, thin=10, flat=True)\n",
        "best_idx = np.argmax([log_posterior(p, x, y) for p in flat_samples]) # the data\n",
        "best_fit = flat_samples[best_idx] # flattens it for easy access\n",
        "cp1, cp2, m1, b1, m2, m3 = best_fit # the values\n",
        "cp1 = np.clip(cp1, 1.0, N - 3 - 1e-6) # ensures not too close to the beginning or the end to not just follow the extremely localized trend\n",
        "cp2 = np.clip(cp2, cp1 + 1e-6, N - 2 - 1e-6) # here as well bbut the second changepoint\n",
        "\n",
        "# forecast settings\n",
        "QUARTERS_AHEAD = 4 # we played aroung with this, but one year made the most logical sense, hence 4 quarters\n",
        "x_forecast = np.arange(N, N + QUARTERS_AHEAD)  # just simply doing the initialization with\n",
        "num_samples = flat_samples.shape[0] # gets the samples and indexing needed terms\n",
        "predictions = np.zeros((num_samples, QUARTERS_AHEAD)) # empty array of the correct\n",
        "\n",
        "# generate predictions for all posterior samples\n",
        "for i, theta in enumerate(flat_samples):\n",
        "    cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s = theta # unpacks the variables\n",
        "    cp1_s = np.clip(cp1_s, 1.0, N - 3 - 1e-6) # same clipping\n",
        "    cp2_s = np.clip(cp2_s, cp1_s + 1e-6, N - 2 - 1e-6) # here aswell\n",
        "    y_pred = piecewise(x_forecast, cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s) # calls the fucntion fro it again\n",
        "    predictions[i] = y_pred # gets the predictions\n",
        "\n",
        "# compute CI from the bayesian inference\n",
        "ci_lower = np.percentile(predictions,2.5,axis=0)\n",
        "ci_upper = np.percentile(predictions,97.5,axis=0)\n",
        "median_forecast = np.median(predictions,axis=0) # this is what is used as the predicted val.\n",
        "\n",
        "# evaluate best-fit model up to 2025\n",
        "x_full = np.arange(N+4)  # through 2025\n",
        "best_fit_model = piecewise(x_full, cp1, cp2, m1, b1, m2, m3) # again putting into the function\n",
        "\n",
        "# plot prediction with CI and mcmc chains to see it better\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(x, y, 'ko-', label='Historical Sharpe Ratio')\n",
        "plt.plot(x_full, best_fit_model, 'r-', lw=2, label='Best Fit') # for the fitted data\n",
        "\n",
        "# plot posterior samples of the predictions, like the CI (kinda)\n",
        "for i, theta in enumerate(flat_samples[:50]):\n",
        "    cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s = theta\n",
        "    cp1_s = np.clip(cp1_s, 1.0, N - 3 - 1e-6) # the same clipping as always\n",
        "    cp2_s = np.clip(cp2_s, cp1_s + 1e-6, N - 2 - 1e-6)\n",
        "    y_pred = piecewise(x_forecast, cp1_s, cp2_s, m1_s, b1_s, m2_s, m3_s) # calls the function once more\n",
        "    plt.plot(x_forecast, y_pred, 'b-', alpha=0.1, lw=1) # plots\n",
        "\n",
        "# plot median forecast and CI\n",
        "plt.plot(x_forecast, median_forecast, 'b',lw=2, label='Median Forecast') # our predicted value\n",
        "plt.fill_between(x_forecast, ci_lower, ci_upper, color='blue', alpha=0.2, label='95% CI') # the corresponding CI\n",
        "\n",
        "# highlight current and predicted point, so know where it is visually\n",
        "plt.scatter([x_forecast[-1]], [median_forecast[-1]], color='orange', zorder=5,\n",
        "            label=f'Predicted: {median_forecast[-1]:.3f}')\n",
        "\n",
        "# vertical lines, pretty simple code but just makes a vertical line for the end of fitting then predicted model endpoint\n",
        "plt.axvline(N - 1, color='green',linestyle=':', label='End of 2023')\n",
        "plt.axvline(x_forecast[-1], color='purple',linestyle='--', label='Start of 2025')\n",
        "plt.xlabel('Quarter')\n",
        "plt.ylabel('Sharpe Ratio')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# to observe the exact values for writing the report, all fust referencing the correct stuff above\n",
        "print(f\"Predicted Sharpe Ratio at Q19: {median_forecast[-1]:.3f}\")\n",
        "print(f\"95% Confidence Interval: [{ci_lower[-1]:.3f}, {ci_upper[-1]:.3f}]\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7d2-K5lmxtTF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}